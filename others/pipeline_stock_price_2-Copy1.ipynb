{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b323c1a4-7086-4c9f-9c0e-bd4998bf691d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install packages\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp.v2 import dsl, compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, component, importer)\n",
    "from typing import NamedTuple\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import gcsfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cdf18e3-3f5c-4348-a86f-623635e30320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"datapath-deploy-api-v1-434102\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://demo_vertext_01\"  # @param {type:\"string\"}\n",
    "SERVICE_ACCOUNT = \"dev-mlops-vertex@datapath-deploy-api-v1-434102.iam.gserviceaccount.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88339021-1f94-497c-916c-f330857f4cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}\n",
    "PIPELINE_ROOT = \"{}/output_info\".format(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f7f91e7-5563-4e0b-89eb-d221ee08e9c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c56661-a9a1-4435-8820-7b259b6c1313",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5a8c04-cad1-46c0-bc1d-9a57c563c6aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", packages_to_install=[\"pandas\", \"google-cloud-storage\"])\n",
    "def preprocess_data(\n",
    "    gcs_bucket_name: str,\n",
    "    source_blob_name: str,\n",
    "    proccesed_blob_name: str,\n",
    "    # output_dataset: OutputPath[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocessing step that downloads a CSV from GCS, processes it, and outputs the result.\n",
    "    \"\"\"\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import io\n",
    "    \n",
    "    # Crear cliente de GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(gcs_bucket_name)\n",
    "    \n",
    "    # Descargar el archivo CSV del bucket    \n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    downloaded_file = blob.download_as_bytes()\n",
    "\n",
    "    # Leer el CSV con pandas desde los bytes descargados\n",
    "    \n",
    "    dataset = pd.read_csv(io.BytesIO(downloaded_file))\n",
    "    \n",
    "    \n",
    "    # Guardar el DataFrame transformado en un archivo CSV en memoria\n",
    "    output_buffer = io.BytesIO()\n",
    "    dataset.to_csv(output_buffer, index=False)\n",
    "    \n",
    "    # Subir el archivo CSV transformado de nuevo al bucket\n",
    "    output_buffer.seek(0)  # Regresar el puntero al inicio del archivo en memoria\n",
    "    new_blob = bucket.blob(proccesed_blob_name)\n",
    "    new_blob.upload_from_file(output_buffer, content_type='text/csv')    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c97f896-cda1-41b5-9018-3260fa0c32ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", packages_to_install=[\"pandas\", \"google-cloud-storage\"])\n",
    "def preprocess_data_2(\n",
    "    gcs_bucket_name: str,\n",
    "    source_blob_name: str,\n",
    "    proccesed_blob_name: str,\n",
    "    output_dataset: Output[Dataset]\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Preprocessing step that downloads a CSV from GCS, processes it, and outputs the result.\n",
    "    \"\"\"\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import io\n",
    "    \n",
    "    # Crear cliente de GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(gcs_bucket_name)\n",
    "    \n",
    "    # Descargar el archivo CSV del bucket    \n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    downloaded_file = blob.download_as_bytes()\n",
    "\n",
    "    # Leer el CSV con pandas desde los bytes descargados\n",
    "    \n",
    "    dataset = pd.read_csv(io.BytesIO(downloaded_file))\n",
    "    \n",
    "    # Renombrar columnas específicas\n",
    "    dataset.rename(columns={'Fecha': 'Date', \n",
    "                                'Último': 'Close', \n",
    "                                'Apertura': 'Open', \n",
    "                                'Máximo': 'High', \n",
    "                                'Mínimo': 'Low',}, inplace=True)\n",
    "    \n",
    "    \n",
    "    # Reemplazar los puntos por guiones en la columna 'fecha'\n",
    "    dataset['Date'] = dataset['Date'].str.replace('.', '-', regex=False)\n",
    "    \n",
    "    # Become Date feature from object to date format\n",
    "    dataset['Date'] = pd.to_datetime(dataset['Date'], format='%d-%m-%Y')    \n",
    "    \n",
    "    # Función para transformar el formato de texto a float\n",
    "    def transformar_a_float(valor):\n",
    "        valor = valor.replace('.', '')  # Eliminar los puntos de miles\n",
    "        valor = valor.replace(',', '.')  # Reemplazar la coma decimal por un punto\n",
    "        return float(valor)  # Convertir a float\n",
    "\n",
    "    # Aplicar la función a la columna\n",
    "    dataset['Open'] = dataset['Open'].apply(transformar_a_float)\n",
    "    dataset['High'] = dataset['High'].apply(transformar_a_float)\n",
    "    dataset['Low'] = dataset['Low'].apply(transformar_a_float)\n",
    "    dataset['Close'] = dataset['Close'].apply(transformar_a_float)    \n",
    "    \n",
    "    \n",
    "    # Guardar el DataFrame transformado en un archivo CSV en memoria\n",
    "    # output_buffer = io.BytesIO()\n",
    "    dataset.to_csv(f\"{output_dataset.path}.csv\", index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d497fefc-f333-4ce7-8a0d-dfaaaa066649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", \n",
    "           packages_to_install=[\n",
    "               \"scikit-learn\",\n",
    "               \"pandas\",\n",
    "               \"keras\",\n",
    "               \"numpy\",\n",
    "               \"tensorflow\",\n",
    "               \"joblib\"\n",
    "               ])\n",
    "def training_model(input_df: Input[Dataset],\n",
    "                   name_file_model: str,\n",
    "                   output_model: Output[Model]\n",
    "                  ):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from joblib import dump\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from sklearn.preprocessing import PowerTransformer\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM, Dense    \n",
    "    \n",
    "    dataset_transformed = pd.read_csv(f\"{input_df.path}.csv\")\n",
    "    # print(dataset_transformed.head(61))\n",
    "    #print(dataset_transformed.columns)\n",
    "    \n",
    "    # Filtrar datos de entrenamiento (2002-2022)\n",
    "    train_data = dataset_transformed[(dataset_transformed['Date'] >= '2002-01-01') & (dataset_transformed['Date'] <= '2021-12-31')]\n",
    "    #print(train_data.head())\n",
    "    # Filtrar datos de prueba (2023 en adelante)\n",
    "    test_data = dataset_transformed[dataset_transformed['Date'] >= '2022-01-01']\n",
    "    #print(test_data.head())\n",
    "    \n",
    "    # Inicializar el escalador\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "    # Ajustar el escalador a la columna 'Close' en los datos de entrenamiento\n",
    "    train_data['Close'] = min_max_scaler.fit_transform(train_data[['Close']])\n",
    "\n",
    "    # Transformar la columna 'Close' en los datos de prueba\n",
    "    #test_data['Close'] = scaler.transform(test_data[['Close']])\n",
    "\n",
    "    print(\"Datos de entrenamiento con 'Close' escalado:\")\n",
    "    print(train_data['Close'].head(61))\n",
    "\n",
    "    dataset_train_normalized = train_data['Close'].values\n",
    "    #print(type(dataset_train_normalized))\n",
    "    #print(dataset_train_normalized.shape)  # Esto debe devolver algo como (n_filas, 1)\n",
    "    #print(dataset_train_normalized.head())  # Esto debe devolver algo como (n_filas, 1)\n",
    "    #print(dataset_train_normalized.info())  # Esto debe devolver algo como (n_filas, 1)    \n",
    "\n",
    "    \n",
    "    dataset_test = test_data[['Close']]\n",
    "    print(type(dataset_test))    \n",
    "    print(\"ok training and test set filtered!!\")\n",
    "    \n",
    "\n",
    "    \n",
    "    # dataset_train_processed = pd.read_csv(\"/Users/michaelandr/Desktop/airflow_deployment_ml/dags/data/train/processed_training_set_ISA_Historical_Info.csv\")\n",
    "    # La red LSTM tendrá como entrada \"time_step\" datos consecutivos, y como salida 1 dato (la predicción a\n",
    "    # partir de esos \"time_step\" datos). Se conformará de esta forma el set de entrenamiento\n",
    "    time_step = 60\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    m = len(dataset_train_normalized)\n",
    "\n",
    "    for i in range(time_step, m):\n",
    "        #print(f\"Procesando fila {i}\")\n",
    "        # X: bloques de \"time_step\" datos: 0-time_step, 1-time_step+1, 2-time_step+2, etc\n",
    "        #X_train.append(dataset_train_normalized['Close'].iloc[i-time_step:i, 0].values)\n",
    "        X_train.append(dataset_train_normalized[i-60:i])\n",
    "        Y_train.append(dataset_train_normalized[i])\n",
    "        \n",
    "        # Y: el siguiente dato\n",
    "        #Y_train.append(dataset_train_normalized.iloc[i,0])\n",
    "        \n",
    "    X_train, Y_train = np.array(X_train), np.array(Y_train)     \n",
    "    print(\"Conversion de arrays exitosa\") \n",
    "    # Reshape X_train para que se ajuste al modelo en Keras\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "    # Valores iniciales\n",
    "    dim_entrada = (X_train.shape[1],1) # 60 datos de una feature\n",
    "    dim_salida = 1\n",
    "    na = 50\n",
    "\n",
    "    # Crear el modelo\n",
    "    print(\"Inicio creacion red lstm\")\n",
    "    model_lstm = Sequential()\n",
    "\n",
    "    # Añadir la capa LSTM\n",
    "    model_lstm.add(LSTM(units=na, return_sequences=True, input_shape= dim_entrada))\n",
    "    model_lstm.add(LSTM(units=na))\n",
    "\n",
    "    # Añadir una capa densa para la salida\n",
    "    model_lstm.add(Dense(dim_salida))\n",
    "\n",
    "    # Compilar el modelo\n",
    "    print(\"compilacion\")    \n",
    "    model_lstm.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "\n",
    "    # Resumen del modelo\n",
    "    model_lstm.summary()\n",
    "\n",
    "    print(\"inicio training\")\n",
    "    # Train the model\n",
    "    model_lstm.fit(X_train, Y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
    "    print(\"Training had finished!\")\n",
    "\n",
    "        \n",
    "    # Guardar el modelo en el formato nativo de Keras en la ruta de salida proporcionada por KFP\n",
    "    #output_path = os.path.join(output_model, f\"{name_file_model}.keras\")\n",
    "    print(output_model.path)\n",
    "    model_lstm.save(f\"{output_model.path}.keras\")\n",
    "\n",
    "    #print(f\"Modelo guardado en: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb0e855-9e2d-4b0e-924e-4edb12f81ce9",
   "metadata": {},
   "source": [
    "#### Definir un pipeline de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "761998c1-d4dc-468f-ae90-86c7bf43e982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"test-pipeline-2\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def csv_preprocessing_pipeline(\n",
    "    project: str = \"datapath-deploy-api-v1-434102\",\n",
    "    gcp_region: str = \"us-central1\"):\n",
    "    \n",
    "    # Definimos una ruta de salida en el bucket de GCS para el dataset procesado\n",
    "    # output_path = \"gs://demo_vertext_01/output_info/processed_data.csv\"\n",
    "\n",
    "    preprocess_task = preprocess_data_2(\n",
    "       gcs_bucket_name=\"demo_vertext_01\",\n",
    "       source_blob_name = \"raw_info/ISA_Historical_Info_2002_2024.csv\",\n",
    "       proccesed_blob_name = \"output_info/processed_ISA_Historical_Info_2002_2024.csv\"\n",
    "        # output_dataset_path=output_path  # Este valor será gestionado automáticamente por KFP\n",
    "    )\n",
    "    \n",
    "    training_model(preprocess_task.output, \"model_lstm\").after(preprocess_task)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f2596-7ede-4d5e-80bd-8a6b0d342996",
   "metadata": {},
   "source": [
    "#### Compilar el pipeline de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f339b89-580c-46ac-adef-dcf8f8d44bec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/dev/lib/python3.8/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func = csv_preprocessing_pipeline,\n",
    "    package_path = \"pipeline_demo_test_3.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8560-6641-43f7-894d-1257db105937",
   "metadata": {},
   "source": [
    "#### Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45688141-cd95-4390-a2d5-8da06ffbb12d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit pipeline job ....\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/172483762390/locations/us-central1/pipelineJobs/test-pipeline-2-20241018214633\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/172483762390/locations/us-central1/pipelineJobs/test-pipeline-2-20241018214633')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/test-pipeline-2-20241018214633?project=172483762390\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project = \"datapath-deploy-api-v1-434102\", location = \"us-central1\")\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name =\"test-pipeline-job-training\",\n",
    "    template_path = \"pipeline_demo_test_3.json\",\n",
    "    pipeline_root = PIPELINE_ROOT,\n",
    "    enable_caching = False,\n",
    "    project = \"datapath-deploy-api-v1-434102\",\n",
    "    location = \"us-central1\"\n",
    "    )\n",
    "\n",
    "print(\"submit pipeline job ....\")\n",
    "job.submit(\"dev-mlops-vertex@datapath-deploy-api-v1-434102.iam.gserviceaccount.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08974c7-221b-452a-a56c-e3a678d93c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b475648-584b-452a-ae8f-43dd39208f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "dev",
   "name": "common-cpu.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m125"
  },
  "kernelspec": {
   "display_name": "Python 3.8 (dev) (Local)",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
