{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37b4518f-61fa-48c0-9187-273ef476e3c2",
   "metadata": {},
   "source": [
    "### GCP ML - Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b323c1a4-7086-4c9f-9c0e-bd4998bf691d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install packages\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp.v2 import dsl, compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, component, importer)\n",
    "from typing import NamedTuple\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import gcsfs\n",
    "from google_cloud_pipeline_components.v1.vertex_notification_email import VertexNotificationEmailOp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257be08a-17aa-46f9-bb04-851e27747808",
   "metadata": {},
   "source": [
    "#### Declare variables and pipeline root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cdf18e3-3f5c-4348-a86f-623635e30320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"datapath-deploy-api-v1-434102\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://demo_vertext_01\"  # @param {type:\"string\"}\n",
    "SERVICE_ACCOUNT = \"dev-mlops-vertex@datapath-deploy-api-v1-434102.iam.gserviceaccount.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88339021-1f94-497c-916c-f330857f4cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}\n",
    "PIPELINE_ROOT = \"{}/output_info\".format(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49715c5d-66a4-451a-84d8-e41b82bdcde7",
   "metadata": {},
   "source": [
    "#### Initialize AIPlatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f7f91e7-5563-4e0b-89eb-d221ee08e9c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c56661-a9a1-4435-8820-7b259b6c1313",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### First component: Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c97f896-cda1-41b5-9018-3260fa0c32ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", packages_to_install=[\"pandas\", \"google-cloud-storage\"])\n",
    "def preprocess_data(\n",
    "    gcs_bucket_name: str,\n",
    "    source_blob_name: str,\n",
    "    proccesed_blob_name: str,\n",
    "    output_dataset: Output[Dataset]\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Preprocessing step that downloads a CSV from GCS, processes it, and outputs the result.\n",
    "    \"\"\"\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import io\n",
    "    \n",
    "    # Crear cliente de GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(gcs_bucket_name)\n",
    "    \n",
    "    # Descargar el archivo CSV del bucket    \n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    downloaded_file = blob.download_as_bytes()\n",
    "\n",
    "    # Leer el CSV con pandas desde los bytes descargados\n",
    "    dataset = pd.read_csv(io.BytesIO(downloaded_file))\n",
    "    \n",
    "    # Renombrar columnas específicas\n",
    "    dataset.rename(columns={'Fecha': 'Date', \n",
    "                                'Último': 'Close', \n",
    "                                'Apertura': 'Open', \n",
    "                                'Máximo': 'High', \n",
    "                                'Mínimo': 'Low',}, inplace=True)\n",
    "    \n",
    "    # Reemplazar los puntos por guiones en la columna 'fecha'\n",
    "    dataset['Date'] = dataset['Date'].str.replace('.', '-', regex=False)\n",
    "    \n",
    "    # Become Date feature from object to date format\n",
    "    dataset['Date'] = pd.to_datetime(dataset['Date'], format='%d-%m-%Y')    \n",
    "    \n",
    "    # Función para transformar el formato de texto a float\n",
    "    def transformar_a_float(valor):\n",
    "        valor = valor.replace('.', '')  # Eliminar los puntos de miles\n",
    "        valor = valor.replace(',', '.')  # Reemplazar la coma decimal por un punto\n",
    "        return float(valor)  # Convertir a float\n",
    "\n",
    "    # Aplicar la función a la columna\n",
    "    dataset['Open'] = dataset['Open'].apply(transformar_a_float)\n",
    "    dataset['High'] = dataset['High'].apply(transformar_a_float)\n",
    "    dataset['Low'] = dataset['Low'].apply(transformar_a_float)\n",
    "    dataset['Close'] = dataset['Close'].apply(transformar_a_float)    \n",
    "    \n",
    "    \n",
    "    # Guardar el DataFrame transformado en un archivo CSV en memoria\n",
    "    dataset.to_csv(f\"{output_dataset.path}.csv\", index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f299f84-26c9-4dd6-8693-78e2b1e5c2d6",
   "metadata": {},
   "source": [
    "#### Second component: Train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d497fefc-f333-4ce7-8a0d-dfaaaa066649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", \n",
    "           packages_to_install=[\n",
    "               \"scikit-learn\",\n",
    "               \"pandas\",\n",
    "               \"keras\",\n",
    "               \"numpy\",\n",
    "               \"tensorflow\",\n",
    "               \"joblib\"\n",
    "               ])\n",
    "def training_model(input_df: Input[Dataset],\n",
    "                   name_file_model: str,\n",
    "                   output_model: Output[Model]\n",
    "                  ):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from joblib import dump\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from sklearn.preprocessing import PowerTransformer\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM, Dense    \n",
    "    \n",
    "    dataset_transformed = pd.read_csv(f\"{input_df.path}.csv\")\n",
    "    \n",
    "    # Filtrar datos de entrenamiento (2002-2022)\n",
    "    train_data = dataset_transformed[(dataset_transformed['Date'] >= '2002-01-01') & (dataset_transformed['Date'] <= '2021-12-31')]\n",
    "\n",
    "    # Filtrar datos de prueba (2023 en adelante)\n",
    "    test_data = dataset_transformed[dataset_transformed['Date'] >= '2022-01-01']\n",
    "    #print(test_data.head())\n",
    "    \n",
    "    # Inicializar el escalador\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "    # Ajustar el escalador a la columna 'Close' en los datos de entrenamiento\n",
    "    train_data['Close'] = min_max_scaler.fit_transform(train_data[['Close']])\n",
    "\n",
    "    print(\"Datos de entrenamiento con 'Close' escalado:\")\n",
    "    print(train_data['Close'].head(3))\n",
    "\n",
    "    dataset_train_normalized = train_data['Close'].values\n",
    "    \n",
    "    dataset_test = test_data[['Close']]\n",
    "    print(\"ok training and test set filtered!!\")\n",
    "    \n",
    "    # dataset_train_processed = pd.read_csv(\"/Users/michaelandr/Desktop/airflow_deployment_ml/dags/data/train/processed_training_set_ISA_Historical_Info.csv\")\n",
    "    # La red LSTM tendrá como entrada \"time_step\" datos consecutivos, y como salida 1 dato (la predicción a\n",
    "    # partir de esos \"time_step\" datos). Se conformará de esta forma el set de entrenamiento\n",
    "    time_step = 60\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    m = len(dataset_train_normalized)\n",
    "\n",
    "    for i in range(time_step, m):\n",
    "        # X: bloques de \"time_step\" datos: 0-time_step, 1-time_step+1, 2-time_step+2, etc\n",
    "        X_train.append(dataset_train_normalized[i-60:i])\n",
    "        Y_train.append(dataset_train_normalized[i])\n",
    "                \n",
    "    X_train, Y_train = np.array(X_train), np.array(Y_train)     \n",
    "    print(\"Conversion de arrays exitosa\") \n",
    "    \n",
    "    # Reshape X_train para que se ajuste al modelo en Keras\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "    # Valores iniciales\n",
    "    dim_entrada = (X_train.shape[1],1) # 60 datos de una feature\n",
    "    dim_salida = 1\n",
    "    na = 50\n",
    "\n",
    "    # Crear el modelo\n",
    "    print(\"Inicio creacion red lstm\")\n",
    "    model_lstm = Sequential()\n",
    "\n",
    "    # Añadir la capa LSTM\n",
    "    model_lstm.add(LSTM(units=na, return_sequences=True, input_shape= dim_entrada))\n",
    "    model_lstm.add(LSTM(units=na))\n",
    "\n",
    "    # Añadir una capa densa para la salida\n",
    "    model_lstm.add(Dense(dim_salida))\n",
    "\n",
    "    # Compilar el modelo\n",
    "    print(\"compilacion\")    \n",
    "    model_lstm.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "\n",
    "    # Resumen del modelo\n",
    "    model_lstm.summary()\n",
    "\n",
    "    print(\"inicio training\")\n",
    "    # Train the model\n",
    "    model_lstm.fit(X_train, Y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
    "    print(\"Training had finished!\")\n",
    "\n",
    "    # Guardar el modelo en el formato nativo de Keras en la ruta de salida proporcionada por KFP\n",
    "    print(output_model.path)\n",
    "    model_lstm.save(f\"{output_model.path}.keras\")\n",
    "\n",
    "    print(f\"Modelo guardado en: {output_model.path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc046f7b-67e1-4c17-bfdd-2102b8c10d2d",
   "metadata": {},
   "source": [
    "#### Third component:Make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c027032-0938-4ab8-ac6f-18e511f088df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", \n",
    "           packages_to_install=[\n",
    "               \"scikit-learn\",\n",
    "               \"pandas\",\n",
    "               \"keras\",\n",
    "               \"numpy\",\n",
    "               \"tensorflow\",\n",
    "               \"joblib\"\n",
    "               ])\n",
    "def make_predictions(input_model: Input[Model],\n",
    "                     gcs_bucket_name: str,\n",
    "                     source_blob_name: str,\n",
    "                     proccesed_blob_name: str):\n",
    "    \"\"\" Import keras model and make predictions based on csv file stored in a bucket. \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from joblib import dump\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from sklearn.preprocessing import PowerTransformer\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from keras.models import Sequential, load_model\n",
    "    from keras.layers import LSTM, Dense  \n",
    "    from google.cloud import storage\n",
    "    import io    \n",
    "    \n",
    "    # Crear cliente de GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(gcs_bucket_name)\n",
    "    \n",
    "    # Descargar el archivo CSV del bucket    \n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    downloaded_file = blob.download_as_bytes()\n",
    "\n",
    "    # Leer el CSV con pandas desde los bytes descargados\n",
    "    dataset_predictions = pd.read_csv(io.BytesIO(downloaded_file))\n",
    "    dataset_predictions = dataset_predictions.drop(columns=[\"Date\", \"Open\", \"High\", \"Low\"])\n",
    "    \n",
    "\n",
    "    # Inicializar el escalador\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "    # Ajustar el escalador a la columna 'Close' en los datos de entrenamiento\n",
    "    dataset_predictions['Close'] = min_max_scaler.fit_transform(dataset_predictions[['Close']])\n",
    "\n",
    "    print(\"Datos productivos escalados:\")\n",
    "    print(dataset_predictions['Close'].head(3))\n",
    "    \n",
    "    # Cargar el modelo guardado en formato .keras\n",
    "    print(\"Inicio cargue del modelo productivo:\")\n",
    "    modelo_lstm = load_model(f\"{input_model.path}.keras\")\n",
    "    print(\"Fin del cargue del modelo productivo:\")\n",
    "    \n",
    "    # Convertir los datos a un formato adecuado para la predicción de LSTM\n",
    "    dataset_predictions_formated = np.reshape(dataset_predictions, (1, dataset_predictions.shape[0], 1))\n",
    "    print(f\"dataset_predictions_formated: {dataset_predictions_formated.shape}\")\n",
    "    \n",
    "    # Hacer la predicción\n",
    "    prediccion_escalada = modelo_lstm.predict(dataset_predictions_formated)\n",
    "\n",
    "    # Invertir la transformación de escala (para volver a los valores originales)\n",
    "    prediccion_final = min_max_scaler.inverse_transform(prediccion_escalada)\n",
    "\n",
    "    print(f\"Predicción: {prediccion_final[0][0]}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb0e855-9e2d-4b0e-924e-4edb12f81ce9",
   "metadata": {},
   "source": [
    "#### Definir un pipeline de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "761998c1-d4dc-468f-ae90-86c7bf43e982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"test-pipeline-2\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def csv_preprocessing_pipeline(\n",
    "    project: str = \"datapath-deploy-api-v1-434102\",\n",
    "    gcp_region: str = \"us-central1\"):\n",
    "    \n",
    "    notify_email_task = VertexNotificationEmailOp(recipients=[\"michael.morapp@gmail.com\"])\n",
    "\n",
    "    with dsl.ExitHandler(notify_email_task):\n",
    "\n",
    "        # Preproceso de datos        \n",
    "        preprocess_task = preprocess_data(\n",
    "           gcs_bucket_name=\"demo_vertext_01\",\n",
    "           source_blob_name = \"raw_info/ISA_Historical_Info_2002_2024.csv\",\n",
    "           proccesed_blob_name = \"output_info/processed_ISA_Historical_Info_2002_2024.csv\"\n",
    "        )\n",
    "\n",
    "        preprocess_task.set_display_name(\"Preprocessing Data has finished\")\n",
    "\n",
    "        modeling_task = training_model(preprocess_task.output, \"model_lstm\").after(preprocess_task)\n",
    "        modeling_task.set_display_name(\"Training Model has finished\")\n",
    "\n",
    "        predictions_task = make_predictions(modeling_task.output, \n",
    "           gcs_bucket_name=\"demo_vertext_01\",\n",
    "           source_blob_name = \"prod_info/ISA_Historical_Info_Prod.csv\",\n",
    "           proccesed_blob_name = \"prod_info/prediction_ISA.csv\"                     \n",
    "        ).after(modeling_task)  \n",
    "\n",
    "        predictions_task.set_display_name(\"Making Predictions has finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f2596-7ede-4d5e-80bd-8a6b0d342996",
   "metadata": {},
   "source": [
    "#### Compilar el pipeline de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f339b89-580c-46ac-adef-dcf8f8d44bec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/dev/lib/python3.8/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func = csv_preprocessing_pipeline,\n",
    "    package_path = \"pipeline_demo_test.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8560-6641-43f7-894d-1257db105937",
   "metadata": {},
   "source": [
    "#### Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45688141-cd95-4390-a2d5-8da06ffbb12d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit pipeline job ....\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/172483762390/locations/us-central1/pipelineJobs/test-pipeline-2-20241022032331\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/172483762390/locations/us-central1/pipelineJobs/test-pipeline-2-20241022032331')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/test-pipeline-2-20241022032331?project=172483762390\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project = \"datapath-deploy-api-v1-434102\", location = \"us-central1\")\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name =\"test-pipeline-job-training\",\n",
    "    template_path = \"pipeline_demo_test.json\",\n",
    "    pipeline_root = PIPELINE_ROOT,\n",
    "    enable_caching = True,\n",
    "    project = \"datapath-deploy-api-v1-434102\",\n",
    "    location = \"us-central1\"\n",
    "    )\n",
    "\n",
    "print(\"Submit pipeline job ....\")\n",
    "job.submit(\"dev-mlops-vertex@datapath-deploy-api-v1-434102.iam.gserviceaccount.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08974c7-221b-452a-a56c-e3a678d93c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b475648-584b-452a-ae8f-43dd39208f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "dev",
   "name": "common-cpu.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m125"
  },
  "kernelspec": {
   "display_name": "Python 3.8 (dev) (Local)",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
